{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "toxic-corps",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-18 16:46:14,708\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-05-18 16:46:14,715\tWARNING services.py:1726 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 8036564992 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=8.78gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "usage: ipykernel_launcher.py [-h] [--stop-iters STOP_ITERS]\n",
      "                             [--stop-timesteps STOP_TIMESTEPS]\n",
      "                             [--stop-reward STOP_REWARD] [--ttl_var TTL_VAR]\n",
      "                             [--cpt CPT] [--algo ALGO]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/exo-info/.local/share/jupyter/runtime/kernel-19576497-46ae-41ec-b128-c6aa204285b7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exo-info/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.agents import ddpg\n",
    "from ray.rllib.agents import a3c\n",
    "\n",
    "\n",
    "\n",
    "from ray.tune import grid_search\n",
    "from my_env import ContentCaching\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ret_lst(cpt):\n",
    "    string1 =  'data/listfile_evol'+str(cpt)+'.data' #_evol'+ , _pos'+\n",
    "    with open(string1, 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "        lst = pickle.load(filehandle)\n",
    "    return lst\n",
    "\n",
    "def ret_nei(cpt):\n",
    "    string2 = 'data/nei_tab_pos'+str(cpt)+'.data'\n",
    "    with open(string2, 'rb') as filehandle:\n",
    "        # read the data as binary data stream\n",
    "        nei_tab = pickle.load(filehandle)\n",
    "    return nei_tab\n",
    "\n",
    "class customExperimentClass():\n",
    "\n",
    "    def __init__(self,ttl_var, cpt, variable, stop_iters=2, stop_timesteps=990000000, stop_reward=0.00001):#\n",
    "\n",
    "        self.env = ContentCaching#gym.make(\"ContentCaching-v0\")\n",
    "        self.config_train = {\n",
    "                        \"env\": ContentCaching,\n",
    "                        \"env_config\": {\n",
    "                        \"ttl_var\": ttl_var,\n",
    "                        \"variable\": variable,#[8,8,8,4],\n",
    "                        \"nei_tab\": ret_nei(cpt),\n",
    "                        \"lst_tab\": ret_lst(cpt),\n",
    "                        },\n",
    "                        \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "\n",
    "                        \"model\": {\n",
    "                            # By default, the MODEL_DEFAULTS dict above will be used.\n",
    "\n",
    "                            # Change individual keys in that dict by overriding them, e.g.\n",
    "                            \"fcnet_hiddens\": grid_search( [[64, 64, 64]]),\n",
    "                            \"fcnet_activation\": grid_search([\"relu\"]),\n",
    "                            \"vf_share_layers\": False,#True,\n",
    "                        },\n",
    "\n",
    "                        \"lr\": grid_search([1e-2]),  # try different lrs\n",
    "                        \"num_workers\": 0,  # parallelism\n",
    "                        #\"framework\": \"torch\" if args.torch else \"tf\",\n",
    "        }\n",
    "\n",
    "        \n",
    "        self.config_test = {\n",
    "                        \"env\": ContentCaching,\n",
    "                        \"env_config\": {\n",
    "                        \"ttl_var\": ttl_var,\n",
    "                        \"variable\": variable,\n",
    "                        \"nei_tab\": ret_nei(5),\n",
    "                        \"lst_tab\": ret_lst(5),                        \n",
    "\n",
    "                        },\n",
    "                        \"model\": {\n",
    "                            # By default, the MODEL_DEFAULTS dict above will be used.\n",
    "\n",
    "                            # Change individual keys in that dict by overriding them, e.g.\n",
    "                            \"fcnet_hiddens\": [64, 64, 64],\n",
    "                            \"fcnet_activation\": \"sigmoid\",\n",
    "                            \"vf_share_layers\": False,#True,\n",
    "                        },\n",
    "\n",
    "                        \"lr\": [1e-2],  # try different lrs\n",
    "                        #\"num_workers\": 2,  # parallelism\n",
    "                        #\"framework\": \"torch\" if args.torch else \"tf\",\n",
    "        }\n",
    "        self.save_dir = \"~/ray_results\"\n",
    "        self.stop_criteria = {\n",
    "                    \"training_iteration\": stop_iters,#args.stop_iters,\n",
    "                    \"timesteps_total\": stop_timesteps,#args.c,\n",
    "                    \"episode_reward_mean\": stop_reward#args.stop_reward,\n",
    "                    }\n",
    "    \n",
    "    def train(self, algo):\n",
    "        \"\"\"\n",
    "        Train an RLlib IMPALA agent using tune until any of the configured stopping criteria is met.\n",
    "            See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "        :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "            See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "        \"\"\"\n",
    "        if algo == \"ppo\":\n",
    "            analysis = ray.tune.run(ppo.PPOTrainer, config=self.config_train, local_dir=self.save_dir, stop=self.stop_criteria,\n",
    "                               checkpoint_at_end=True)\n",
    "        if algo == \"ddpg\":\n",
    "            analysis = ray.tune.run(ddpg.DDPGTrainer, config=self.config_train, local_dir=self.save_dir, stop=self.stop_criteria,\n",
    "                                checkpoint_at_end=True)\n",
    "        if algo == \"a3c\":\n",
    "            analysis = ray.tune.run(a3c.A3CTrainer, config=self.config_train, local_dir=self.save_dir, stop=self.stop_criteria,\n",
    "                                checkpoint_at_end=True)\n",
    "        if algo == \"td3\":\n",
    "            analysis = ray.tune.run(ddpg.TD3Trainer, config=self.config_train, local_dir=self.save_dir, stop=self.stop_criteria,\n",
    "                                checkpoint_at_end=True)\n",
    "        if algo == \"appo\":\n",
    "            analysis = ray.tune.run(ppo.APPOTrainer, config=self.config_train, local_dir=self.save_dir, stop=self.stop_criteria,\n",
    "                                checkpoint_at_end=True)\n",
    "\n",
    "        lr = analysis.get_best_config(metric='episode_reward_mean', mode=\"max\")[\"lr\"] \n",
    "        fc_hid = analysis.get_best_config(metric='episode_reward_mean', mode=\"max\")[\"model\"][\"fcnet_hiddens\"] \n",
    "        fc_act = analysis.get_best_config(metric='episode_reward_mean', mode=\"max\")[\"model\"][\"fcnet_activation\"] \n",
    "\n",
    "        # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "        checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode = 'max'),\n",
    "                                                           metric='episode_reward_mean')\n",
    "        # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "\n",
    "        df = analysis.results_df\n",
    "\n",
    "        all_dataframes = analysis.trial_dataframes\n",
    "        print(\"all_dataframes ===== : \", all_dataframes)\n",
    "        print(\"type all_dataframes ===== : \", type(all_dataframes))\n",
    "\n",
    "        print(\"--------------------------------------\")\n",
    "        trials = analysis.trials\n",
    "        print(\"trials ===== : \", trials)\n",
    "        print(\"trials[0] ===== : \", trials[0])\n",
    "        print(\"len trials ===== : \", len(trials))\n",
    "\n",
    "        print(\"type trials ===== : \", type(trials))\n",
    "        #df = analysis.dataframe(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "        #print(\"df ===== : \", type(df))\n",
    "\n",
    "        dfs = analysis.trial_dataframes\n",
    "\n",
    "        # Plot by epoch\n",
    "        ax = None  # This plots everything on the same plot\n",
    "        for d in dfs.values():\n",
    "            ax = d.episode_reward_mean.plot(ax=ax, legend=False)\n",
    "        plot.show()\n",
    "\n",
    "\n",
    "        #dff = pd.DataFrame(df).set_index('Index')\n",
    "        #print(\"dff ===== : \", type(dff))\n",
    "\n",
    "\n",
    "        #print(\"dff ===== : \", dff[\"trial_id\"])\n",
    "        #print(\"dff ===== : \", dff[\"hist_stats/episode_reward\"])\n",
    "\n",
    "        checkpoint_path = checkpoints[0][0]\n",
    "        print(\"Checkpoint path:\", checkpoint_path)\n",
    "        return checkpoint_path, analysis, lr, fc_hid, fc_act\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load a trained RLlib agent from the specified path. Call this before testing a trained agent.\n",
    "        :param path: Path pointing to the agent's saved checkpoint (only used for RLlib agents)\n",
    "        \"\"\"\n",
    "        self.agent = ppo.PPOTrainer(config=self.config)\n",
    "        self.agent.restore(path)\n",
    "\n",
    "    def test(self,algo, path, lr, fc_hid, fc_act):\n",
    "\n",
    "        \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "        # instantiate env class\n",
    "        unused_shared = []\n",
    "        unused_own = []\n",
    "        unsatisfied_shared = []\n",
    "        unsatisfied_own = []\n",
    "\n",
    "        episode_reward = 0\n",
    "        self.config_test[\"num_workers\"] = 0\n",
    "        self.config_test[\"lr\"] = lr\n",
    "        self.config_test['model'][\"fcnet_hiddens\"] = fc_hid\n",
    "        self.config_test['model'][\"fcnet_activation\"] = fc_act\n",
    "\n",
    "        if algo == \"ppo\":\n",
    "            self.agent = ppo.PPOTrainer(config=self.config_test)\n",
    "        if algo == \"ddpg\":\n",
    "            self.agent = ddpg.DDPGTrainer(config=self.config_test)\n",
    "        if algo == \"a3c\":\n",
    "            self.agent = a3c.A3CTrainer(config=self.config_test)\n",
    "        if algo == \"td3\":\n",
    "            self.agent = ddpg.TD3Trainer(config=self.config_test)\n",
    "        if algo == \"appo\":\n",
    "            self.agent = ppo.APPOTrainer(config=self.config_test)\n",
    "\n",
    "        self.agent.restore(path)\n",
    "        env = self.agent.workers.local_worker().env\n",
    "\n",
    "     \n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = self.agent.compute_action(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            unused_shared.append(info[\"unused_shared\"])\n",
    "            unused_own.append(info[\"unused_own\"])\n",
    "            unsatisfied_shared.append(info[\"unsatisfied_shared\"])\n",
    "            unsatisfied_own.append(info[\"unsatisfied_own\"])\n",
    "\n",
    "        return episode_reward, unused_shared, unused_own, unsatisfied_shared, unsatisfied_own\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--stop-iters\", type=int, default= 2)#50)\n",
    "    parser.add_argument(\"--stop-timesteps\", type=int, default=90000000)\n",
    "    parser.add_argument(\"--stop-reward\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--ttl_var\", type=float, default=3)\n",
    "    parser.add_argument(\"--cpt\", type=float, default=1)\n",
    "    parser.add_argument(\"--algo\", type=str, default=\"ppo\")   \n",
    "\n",
    "\n",
    "    ray.shutdown()\n",
    "    ray.init(num_cpus=3)#num_cpus=2, num_gpus=0)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    # Class instance\n",
    "    exper = customExperimentClass(args.ttl_var, args.cpt, [8,8,8,4], args.stop_iters) # ttl_var, cpt, variable\n",
    "\n",
    "    # Train and save for 2 iterations\n",
    "    checkpoint_path, results, lr, fc_hid, fc_act = exper.train(args.algo)\n",
    "    \n",
    "    print(\"------------------------------------------------------------------------------------\")\n",
    "    print(\"------------------------------------------------------------------------------------\")\n",
    "    print(\"------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    # Load saved\n",
    "    #exper.load(checkpoint_path)\n",
    "    # Test loaded\n",
    "    \"\"\"\n",
    "    reward, unused_shared ,unused_own, unsatisfied_shared, unsatisfied_own  = exper.test(args.algo,checkpoint_path, lr, fc_hid, fc_act)\n",
    "   \n",
    "    print(\" info[unused_shared] = \", unused_shared )\n",
    "    print(\" info[unused_own] = \", unused_own )\n",
    "    print(\" info[unsatisfied_shared] = \", unsatisfied_shared )\n",
    "    print(\" info[unsatisfied_own] = \", unsatisfied_own )\n",
    "    print(\" reward = \", reward )\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    " \n",
    "\n",
    "    config=dict(\n",
    "        extra_config,\n",
    "        **{\n",
    "            \"env\": \"BreakoutNoFrameskip-v4\"\n",
    "            if args.use_vision_network else \"CartPole-v0\",\n",
    "            # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "            \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "            \"callbacks\": {\n",
    "                \"on_train_result\": check_has_custom_metric,\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"custom_model\": \"keras_q_model\"\n",
    "                if args.run == \"DQN\" else \"keras_model\"\n",
    "            },\n",
    "            \"framework\": \"tf\",\n",
    "        })\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "functioning-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/exo-info/.local/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2021-05-18 16:42:58,951\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "2021-05-18 16:42:58,955\tWARNING services.py:1726 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 8038137856 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=8.60gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "== Status ==\n",
      "Memory usage on this node: 14.2/31.1 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/3 CPUs, 0/0 GPUs, 0.0/15.64 GiB heap, 0.0/7.82 GiB objects\n",
      "Result logdir: /home/exo-info/ray_results/PPO_2021-05-18_16-43-00\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+--------------------------------+----------+-------+------+--------------------------+-----------------------+\n",
      "| Trial name                     | status   | loc   |   lr | model/fcnet_activation   | model/fcnet_hiddens   |\n",
      "|--------------------------------+----------+-------+------+--------------------------+-----------------------|\n",
      "| PPO_ContentCaching_b99f2_00000 | RUNNING  |       | 0.01 | relu                     | [64, 64, 64]          |\n",
      "+--------------------------------+----------+-------+------+--------------------------+-----------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m WARNING:tensorflow:From /home/exo-info/.local/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m 2021-05-18 16:43:02,374\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m 2021-05-18 16:43:02,374\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m /home/exo-info/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m WARNING:tensorflow:From /home/exo-info/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m 2021-05-18 16:43:06,482\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m 2021-05-18 16:43:22,141\tWARNING deprecation.py:34 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "Result for PPO_ContentCaching_b99f2_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-18_16-43-26\n",
      "  done: false\n",
      "  episode_len_mean: 19.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -840.6305972942216\n",
      "  episode_reward_mean: -939.0681178533422\n",
      "  episode_reward_min: -1044.0752908564393\n",
      "  episodes_this_iter: 210\n",
      "  episodes_total: 210\n",
      "  experiment_id: 8b3ea34170e6486296ca862fe32a5f3d\n",
      "  hostname: exoinfo-Precision-5820-Tower\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 28.665660858154297\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.14173409342765808\n",
      "          model: {}\n",
      "          policy_loss: 0.09421256929636002\n",
      "          total_loss: 264431.40625\n",
      "          vf_explained_var: 0.014208603650331497\n",
      "          vf_loss: 264431.28125\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.29.157.33\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.7551724137931\n",
      "    ram_util_percent: 46.35517241379311\n",
      "  pid: 10665\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1307031983764313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.369054226063693\n",
      "    mean_inference_ms: 1.252399686276093\n",
      "    mean_raw_obs_processing_ms: 0.1512952698257321\n",
      "  time_since_restore: 19.87350845336914\n",
      "  time_this_iter_s: 19.87350845336914\n",
      "  time_total_s: 19.87350845336914\n",
      "  timers:\n",
      "    learn_throughput: 962.902\n",
      "    learn_time_ms: 4154.108\n",
      "    sample_throughput: 255.502\n",
      "    sample_time_ms: 15655.43\n",
      "  timestamp: 1621352606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: b99f2_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.4/31.1 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/3 CPUs, 0/0 GPUs, 0.0/15.64 GiB heap, 0.0/7.82 GiB objects\n",
      "Result logdir: /home/exo-info/ray_results/PPO_2021-05-18_16-43-00\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+--------------------------------+----------+--------------------+------+--------------------------+-----------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                     | status   | loc                |   lr | model/fcnet_activation   | model/fcnet_hiddens   |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|--------------------------------+----------+--------------------+------+--------------------------+-----------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_ContentCaching_b99f2_00000 | RUNNING  | 10.29.157.33:10665 | 0.01 | relu                     | [64, 64, 64]          |      1 |          19.8735 | 4000 | -939.068 |             -840.631 |             -1044.08 |                 19 |\n",
      "+--------------------------------+----------+--------------------+------+--------------------------+-----------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_ContentCaching_b99f2_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-18_16-43-45\n",
      "  done: true\n",
      "  episode_len_mean: 19.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -824.0811166580559\n",
      "  episode_reward_mean: -947.9743968934732\n",
      "  episode_reward_min: -1092.5272430483617\n",
      "  episodes_this_iter: 211\n",
      "  episodes_total: 421\n",
      "  experiment_id: 8b3ea34170e6486296ca862fe32a5f3d\n",
      "  hostname: exoinfo-Precision-5820-Tower\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 28.880329132080078\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011872530914843082\n",
      "          model: {}\n",
      "          policy_loss: 0.0018051927909255028\n",
      "          total_loss: 242284.53125\n",
      "          vf_explained_var: 0.030574720352888107\n",
      "          vf_loss: 242284.515625\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.29.157.33\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.203703703703702\n",
      "    ram_util_percent: 46.38518518518518\n",
      "  pid: 10665\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12717093844485872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 2.3278719245635067\n",
      "    mean_inference_ms: 1.2202989070240573\n",
      "    mean_raw_obs_processing_ms: 0.14747191840239635\n",
      "  time_since_restore: 38.878621339797974\n",
      "  time_this_iter_s: 19.005112886428833\n",
      "  time_total_s: 38.878621339797974\n",
      "  timers:\n",
      "    learn_throughput: 986.181\n",
      "    learn_time_ms: 4056.05\n",
      "    sample_throughput: 260.947\n",
      "    sample_time_ms: 15328.759\n",
      "  timestamp: 1621352625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: b99f2_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 14.4/31.1 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/3 CPUs, 0/0 GPUs, 0.0/15.64 GiB heap, 0.0/7.82 GiB objects\n",
      "Result logdir: /home/exo-info/ray_results/PPO_2021-05-18_16-43-00\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+--------------------------------+----------+--------------------+------+--------------------------+-----------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                     | status   | loc                |   lr | model/fcnet_activation   | model/fcnet_hiddens   |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|--------------------------------+----------+--------------------+------+--------------------------+-----------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_ContentCaching_b99f2_00000 | RUNNING  | 10.29.157.33:10665 | 0.01 | relu                     | [64, 64, 64]          |      2 |          38.8786 | 8000 | -947.974 |             -824.081 |             -1092.53 |                 19 |\n",
      "+--------------------------------+----------+--------------------+------+--------------------------+-----------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 14.4/31.1 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/0 GPUs, 0.0/15.64 GiB heap, 0.0/7.82 GiB objects\n",
      "Result logdir: /home/exo-info/ray_results/PPO_2021-05-18_16-43-00\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+--------------------------------+------------+-------+------+--------------------------+-----------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                     | status     | loc   |   lr | model/fcnet_activation   | model/fcnet_hiddens   |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|--------------------------------+------------+-------+------+--------------------------+-----------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_ContentCaching_b99f2_00000 | TERMINATED |       | 0.01 | relu                     | [64, 64, 64]          |      2 |          38.8786 | 8000 | -947.974 |             -824.081 |             -1092.53 |                 19 |\n",
      "+--------------------------------+------------+-------+------+--------------------------+-----------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m 2021-05-18 16:43:45,890\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"/home/exo-info/.local/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"/home/exo-info/.local/lib/python3.6/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"/home/exo-info/.local/lib/python3.6/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"/usr/lib/python3.6/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"/usr/lib/python3.6/traceback.py\", line 121, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m     type(value), value, tb, limit=limit).format(chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"/usr/lib/python3.6/traceback.py\", line 509, in __init__\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m     capture_locals=capture_locals)\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"/usr/lib/python3.6/traceback.py\", line 351, in extract\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m     linecache.lazycache(filename, f.f_globals)\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m   File \"/home/exo-info/.local/lib/python3.6/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=10665)\u001b[0m SystemExit: 1\n",
      "2021-05-18 16:43:45,992\tINFO tune.py:549 -- Total run time: 45.59 seconds (45.10 seconds for the tuning loop).\n",
      "all_dataframes ===== :  {'/home/exo-info/ray_results/PPO_2021-05-18_16-43-00/PPO_ContentCaching_b99f2_00000_0_lr=0.01,fcnet_activation=relu,fcnet_hiddens=[64, 64, 64]_2021-05-18_16-43-00':    episode_reward_max  ...  info/learner/default_policy/learner_stats/entropy_coeff\n",
      "0         -840.630597  ...                                                0.0      \n",
      "1         -824.081117  ...                                                0.0      \n",
      "\n",
      "[2 rows x 48 columns]}\n",
      "type all_dataframes ===== :  <class 'dict'>\n",
      "--------------------------------------\n",
      "trials ===== :  [PPO_ContentCaching_b99f2_00000]\n",
      "trials[0] ===== :  PPO_ContentCaching_b99f2_00000\n",
      "len trials ===== :  1\n",
      "type trials ===== :  <class 'list'>\n",
      "Checkpoint path: /home/exo-info/ray_results/PPO_2021-05-18_16-43-00/PPO_ContentCaching_b99f2_00000_0_lr=0.01,fcnet_activation=relu,fcnet_hiddens=[64, 64, 64]_2021-05-18_16-43-00/checkpoint_000002/checkpoint-2\n",
      "------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python customclass.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "leading-lesson",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-555bb1fc2ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'analysis' is not defined"
     ]
    }
   ],
   "source": [
    "dfs = analysis.trial_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-adelaide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
